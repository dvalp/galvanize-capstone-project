# NLP Analysis of Washington State Supreme Court Opinions

Galvanize Data Science Immersive Capstone Project using Apache Spark and NLP to analyze Washington State Supreme Court opinions. The data has been made available by the [Free Law Project](https://free.law/) via the [CourtListener](https://www.courtlistener.com/) website. CourtListener provides the data via an API or as a bulk download of all the documents in a compressed archive of individual files, each containing a single JSON object for one court opinion.

[Apache Spark](https://spark.apache.org/) is an engine for large scale data processing that makes it possible to work on large amounts of data in parallel wherever possible. Where Spark does not have native functions to implement machine learning algorithms, it can also run external functions in parallel (in this case functions are imported from Python.

Where functions are not available in Spark, I have primarily made use of the [Natural Language Toolkit (NLTK)](http://www.nltk.org/) and [Scikit-Learn](http://scikit-learn.org/) to process my data.

### Project implementation
The main goal of this project is to provide relevant information from the court opinions. I have chosen to do this in two ways. First, by providing a set of words using tf/idf (CountVectorizer and IDF in Spark) to evaluate their importance within the corpus. Second, I have used the Spark implementation of word2vec to provide a set of vectors which can be used to evaluate document similarity. Using the squared distance method supplied by word2vec has resulted in a set of documents that show remarkable similarities to a single reference document.

The first step was to import the data. The FreeLaw Project conveniently makes the opinions of a court available for download in one large batch archive. Each individual file in the archive contains one single JSON object which describes one opinion. Each case that comes before the court can have multiple opinions; most notably lead, concurring, and dissenting opinions. Although these opinions are generally displayed together on the CourtListener website, they can also be connected via a cluster id that uniquely identifies the case. I chose to leave each opinion as a separate file, leaving the option available to compare opinions on a single case with each other. This also makes it possible to compare a particular type of opinion (ie, dissenting) with each other without the interference of the other opinion types on that case. At least a few cases have more than one dissenting opinion, which may be interesting to explore later. Unfortunately, the original documents often include all the opinions in a single document and it would be very difficult to separate them.

The second step was to limit the data to relevant features. For my first two goals this required preserving both the cluster id (for grouping opinions on a single case) and document id to identify each document uniquely while preserving their relationships to each other. I also included the text of the opinions and the list of opinions cited in each opinion. The text was primarily stored as HTML in one of four columns. I parsed the text from the HTML using [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/). The citations have been stored for future work in creating a graph of the connections between opinions. I have used both squared distance and cosine similarity to measure the difference between document vectors. There are some minor differences, and it is difficult to tell which gives the most accurate measure of document similarity. 

Using Spark's word count and IDF functions, I created tf/idf vectors for each word in each document. I used these vectors to find relevant words in a particular document. As an example, I used the landmark WA Supreme Court opinion [State v. Gunwall](https://www.courtlistener.com//opinion/1390131/state-v-gunwall/). The tf/idf vectors identified words such as cocaine, pen, register, toll, and call as important in this document. These words were in fact central to the case itself. Interesting to note is that the main name in the case (Gunwall) was not recognized as uniquely important to this specific case. This can be explained by the idf weighting which penalizes terms for being too common across documents. State v. Gunwall is a landmark case cited in over 450 other opinions, making the name Gunwall slightly less of a specific identifier, even in the case that bears the name.

For document similarity (given a single document, which other documents are the most similar) I used the Spark implementation of word2vec. Using a vector size of 250, I created a single vector for each document. The Spark implementation includes a squared distance metric for document similarity. The squared distance can be affected by document size (particularly word repetition) to create distance where it is not necessarily present (for example, duplicate information in one document). Therefore, I have also created a cosine similarity measure. AS noted above, it is difficult to tell which is most relevant without reading a large number of documents.

Spark has provided for the creation of a [pipeline](https://spark.apache.org/docs/latest/ml-pipeline.html) object (modeled on the [sklearn pipeline](http://scikit-learn.org/stable/modules/pipeline.html)) to organize the steps in the machine learning process. After defining each model, they can be added to the pipeline and run through the fit and transform process to apply machine learning algorithms to a dataframe. Unfortunately, pyspark UDF (user defined function) methods cannot be added into a pipeline to take advantage of the benefits of running the methods all together. Pyspark does offer an abstract base class, [Transformer](https://github.com/apache/spark/blob/39e2bad6a866d27c3ca594d15e574a1da3ee84cc/python/pyspark/ml/base.py#L71) which can be used to turn a function (via a UDF) into a machine learning transformer that is compatible with the pipeline object. I have used this to create a class ([Stemming_Transformer](https://github.com/dvalp/galvanize-capstone-project/blob/devel/src/ml_transformer.py#L10)) that enables my pipeline to use a stemmer imported from NLTK as part of my pipeline. This does not seem to documented anywhere that is easy to find and most of my inspiration for this came from one [response](http://stackoverflow.com/a/32337101) on StackOverflow. Note that this also requires `from pyspark.ml.util import Identifiable` be defined in addition to `HasInputCol` and `HasOutputCol`.
